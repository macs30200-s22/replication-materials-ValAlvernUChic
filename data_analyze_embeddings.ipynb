{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import gensim \n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_syn0norm(model):\n",
    "    \"\"\"since syn0norm is now depricated\"\"\"\n",
    "    return (model.wv.vectors / np.sqrt((model.wv.vectors ** 2).sum(-1))[..., np.newaxis]).astype(np.float32)\n",
    "\n",
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "    \"\"\"\n",
    "    Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf\n",
    "    Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "\n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "\n",
    "    # patch by Richard So [https://twitter.com/richardjeanso) (thanks!) to update this code for new version of gensim\n",
    "    # base_embed.init_sims(replace=True)\n",
    "    # other_embed.init_sims(replace=True)\n",
    "\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "    # get the (normalized) embedding matrices\n",
    "    base_vecs = calc_syn0norm(in_base_embed)\n",
    "    other_vecs = calc_syn0norm(in_other_embed)\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs)\n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v)\n",
    "    # Replace original array with modified one, i.e. multiplying the embedding matrix by \"ortho\"\n",
    "    other_embed.wv.vectors = (other_embed.wv.vectors).dot(ortho)\n",
    "\n",
    "    return other_embed\n",
    "\n",
    "def intersection_align_gensim(m1, m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.index_to_key)\n",
    "    vocab_m2 = set(m2.wv.index_to_key)\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1 & vocab_m2\n",
    "    if words: common_vocab &= set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1 - common_vocab and not vocab_m2 - common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, \"count\") + m2.wv.get_vecattr(w, \"count\"), reverse=True)\n",
    "    # print(len(common_vocab))\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1, m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.key_to_index[w] for w in common_vocab]\n",
    "        old_arr = m.wv.vectors\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.vectors = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        new_key_to_index = {}\n",
    "        new_index_to_key = []\n",
    "        for new_index, key in enumerate(common_vocab):\n",
    "            new_key_to_index[key] = new_index\n",
    "            new_index_to_key.append(key)\n",
    "        m.wv.key_to_index = new_key_to_index\n",
    "        m.wv.index_to_key = new_index_to_key\n",
    "\n",
    "        print(len(m.wv.key_to_index), len(m.wv.vectors))\n",
    "\n",
    "    return (m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareModels(df, category, sort = True):\n",
    "    \"\"\"If you are using time as your category sorting is important\"\"\"\n",
    "    embeddings_raw = {}\n",
    "    cats = sorted(set(df[category]))\n",
    "    for cat in tqdm(cats):\n",
    "        #This can take a while\n",
    "        print(\"Embedding {}\".format(cat), end = '\\r')\n",
    "        subsetDF = df[df[category] == cat]\n",
    "        #You might want to change the W2V parameters\n",
    "        embeddings_raw[cat] = gensim.models.word2vec.Word2Vec(subsetDF['normalized_sents'].sum(), window=10, workers=5, epochs=10)\n",
    "    #These are much quicker\n",
    "    embeddings_aligned = {}\n",
    "    for catOuter in cats:\n",
    "        embeddings_aligned[catOuter] = [embeddings_raw[catOuter]]\n",
    "        for catInner in cats:\n",
    "            embeddings_aligned[catOuter].append(smart_procrustes_align_gensim(embeddings_aligned[catOuter][-1], embeddings_raw[catInner]))\n",
    "    return embeddings_raw, embeddings_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawEmbeddings, comparedEmbeddings = compareModels(data, 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDivergenceDF(word, embeddingsDict):\n",
    "    \"\"\"\n",
    "    Creates a divergence heatmap that illustrates the temporal degree of change for a given word\n",
    "\n",
    "    Inputs:\n",
    "        word - word of interest\n",
    "        embeddingsDict - aligned embeddings created by compareModels\n",
    "\n",
    "    Returns:\n",
    "        Pandas dataframe containing the values to be plotted\n",
    "    \"\"\"\n",
    "    dists = []\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    dists = {}\n",
    "    for cat in cats:\n",
    "        dists[cat] = []\n",
    "        for embed in embeddingsDict[cat][1:]:\n",
    "            dists[cat].append(np.abs(1 - cosine_similarity(embeddingsDict[cat][0].wv[word].reshape(1, -1),\n",
    "                                                                             embed.wv[word].reshape(1, -1))[0,0]))\n",
    "    return pd.DataFrame(dists, index = cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetWord = 'migrantworker'\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = True) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
