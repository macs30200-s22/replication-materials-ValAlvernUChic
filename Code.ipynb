{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import format_hansard\n",
    "import pandas as pd\n",
    "import json\n",
    "import gensim \n",
    "import sklearn\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Administrator/Desktop/UChicago/Y1Q3/30200/Project/hansard_data/hansard_full 2/12th/output\n"
     ]
    }
   ],
   "source": [
    "cd /Users/Administrator/Desktop/UChicago/Y1Q3/30200/Project/hansard_data/hansard_full 2/12th/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "def scan_folder(parent):\n",
    "    # iterate over all the files in directory 'parent'\n",
    "    for file_name in os.listdir(parent):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            # if it's a txt file, print its name (or do whatever you want)\n",
    "            files.append(\"\".join((parent, \"/\", file_name)))\n",
    "        else:\n",
    "            current_path = \"\".join((parent, \"/\", file_name))\n",
    "            if os.path.isdir(current_path):\n",
    "                # if we're checking a sub-directory, recursively call this method\n",
    "                scan_folder(current_path)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = scan_folder('/Users/Administrator/Desktop/UChicago/Y1Q3/30200/Project/hansard_data/hansard_full 2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013\n",
      "2015\n",
      "2015\n",
      "2015\n",
      "2014\n",
      "2014\n",
      "2013\n",
      "2013\n",
      "2013\n",
      "2015\n",
      "2015\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2015\n",
      "2012\n",
      "2013\n",
      "2013\n",
      "2012\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2013\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2014\n",
      "2013\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2013\n",
      "2014\n",
      "2012\n",
      "2015\n",
      "2014\n",
      "2013\n",
      "2013\n",
      "2012\n",
      "2012\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2015\n",
      "2015\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2014\n",
      "2014\n",
      "2012\n",
      "2012\n",
      "2015\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2012\n",
      "2015\n",
      "2014\n",
      "2013\n",
      "2019\n",
      "2017\n",
      "2018\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2019\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2018\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2016\n",
      "2016\n",
      "2016\n",
      "2018\n",
      "2016\n",
      "2018\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2020\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2016\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2016\n",
      "2018\n",
      "2018\n",
      "2017\n",
      "2019\n",
      "2020\n",
      "2016\n",
      "2019\n",
      "2017\n",
      "2017\n",
      "2020\n",
      "2020\n",
      "2020\n",
      "2017\n",
      "2017\n",
      "2017\n",
      "2020\n",
      "2020\n",
      "2016\n",
      "2019\n",
      "2019\n",
      "2019\n",
      "2019\n",
      "2018\n",
      "2018\n",
      "2017\n",
      "2019\n",
      "2020\n",
      "2019\n",
      "2020\n",
      "2019\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2016\n",
      "2019\n",
      "2016\n",
      "2019\n",
      "2016\n",
      "2020\n",
      "2018\n",
      "2020\n",
      "2019\n",
      "2017\n",
      "2020\n",
      "2020\n",
      "2019\n",
      "2016\n",
      "2020\n",
      "2017\n",
      "2020\n",
      "2020\n",
      "2017\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2016\n",
      "2017\n",
      "2016\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2019\n",
      "2018\n",
      "2018\n",
      "2020\n",
      "2020\n",
      "2020\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2019\n",
      "2019\n",
      "2016\n",
      "2016\n",
      "2020\n",
      "2020\n",
      "2018\n",
      "2018\n",
      "2016\n",
      "2016\n",
      "2018\n",
      "2019\n",
      "2019\n",
      "2019\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2020\n",
      "2021\n",
      "2020\n",
      "2020\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2020\n",
      "2013\n",
      "2015\n",
      "2015\n",
      "2015\n",
      "2014\n",
      "2014\n",
      "2013\n",
      "2013\n",
      "2013\n",
      "2015\n",
      "2015\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2015\n",
      "2012\n",
      "2013\n",
      "2013\n",
      "2012\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2013\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2014\n",
      "2013\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2013\n",
      "2014\n",
      "2012\n",
      "2015\n",
      "2014\n",
      "2013\n",
      "2013\n",
      "2012\n",
      "2012\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2015\n",
      "2015\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2014\n",
      "2014\n",
      "2012\n",
      "2012\n",
      "2015\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2012\n",
      "2015\n",
      "2014\n",
      "2013\n",
      "2019\n",
      "2017\n",
      "2018\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2019\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2018\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2016\n",
      "2016\n",
      "2016\n",
      "2018\n",
      "2016\n",
      "2018\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2020\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2016\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2016\n",
      "2018\n",
      "2018\n",
      "2017\n",
      "2019\n",
      "2020\n",
      "2016\n",
      "2019\n",
      "2017\n",
      "2017\n",
      "2020\n",
      "2020\n",
      "2020\n",
      "2017\n",
      "2017\n",
      "2017\n",
      "2020\n",
      "2020\n",
      "2016\n",
      "2019\n",
      "2019\n",
      "2019\n",
      "2019\n",
      "2018\n",
      "2018\n",
      "2017\n",
      "2019\n",
      "2020\n",
      "2019\n",
      "2020\n",
      "2019\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2016\n",
      "2019\n",
      "2016\n",
      "2019\n",
      "2016\n",
      "2020\n",
      "2018\n",
      "2020\n",
      "2019\n",
      "2017\n",
      "2020\n",
      "2020\n",
      "2019\n",
      "2016\n",
      "2020\n",
      "2017\n",
      "2020\n",
      "2020\n",
      "2017\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2016\n",
      "2017\n",
      "2016\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2019\n",
      "2018\n",
      "2018\n",
      "2020\n",
      "2020\n",
      "2020\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2019\n",
      "2019\n",
      "2016\n",
      "2016\n",
      "2020\n",
      "2020\n",
      "2018\n",
      "2018\n",
      "2016\n",
      "2016\n",
      "2018\n",
      "2019\n",
      "2019\n",
      "2019\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2020\n",
      "2021\n",
      "2020\n",
      "2020\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2020\n",
      "2013\n",
      "2015\n",
      "2015\n",
      "2015\n",
      "2014\n",
      "2014\n",
      "2013\n",
      "2013\n",
      "2013\n",
      "2015\n",
      "2015\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2015\n",
      "2012\n",
      "2013\n",
      "2013\n",
      "2012\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2013\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2014\n",
      "2013\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2013\n",
      "2014\n",
      "2012\n",
      "2015\n",
      "2014\n",
      "2013\n",
      "2013\n",
      "2012\n",
      "2012\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2015\n",
      "2015\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2014\n",
      "2014\n",
      "2012\n",
      "2012\n",
      "2015\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2012\n",
      "2015\n",
      "2014\n",
      "2013\n",
      "2019\n",
      "2017\n",
      "2018\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2019\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2018\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2016\n",
      "2016\n",
      "2016\n",
      "2018\n",
      "2016\n",
      "2018\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2020\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2016\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2016\n",
      "2018\n",
      "2018\n",
      "2017\n",
      "2019\n",
      "2020\n",
      "2016\n",
      "2019\n",
      "2017\n",
      "2017\n",
      "2020\n",
      "2020\n",
      "2020\n",
      "2017\n",
      "2017\n",
      "2017\n",
      "2020\n",
      "2020\n",
      "2016\n",
      "2019\n",
      "2019\n",
      "2019\n",
      "2019\n",
      "2018\n",
      "2018\n",
      "2017\n",
      "2019\n",
      "2020\n",
      "2019\n",
      "2020\n",
      "2019\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2016\n",
      "2019\n",
      "2016\n",
      "2019\n",
      "2016\n",
      "2020\n",
      "2018\n",
      "2020\n",
      "2019\n",
      "2017\n",
      "2020\n",
      "2020\n",
      "2019\n",
      "2016\n",
      "2020\n",
      "2017\n",
      "2020\n",
      "2020\n",
      "2017\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2016\n",
      "2017\n",
      "2016\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2019\n",
      "2018\n",
      "2018\n",
      "2020\n",
      "2020\n",
      "2020\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2019\n",
      "2019\n",
      "2016\n",
      "2016\n",
      "2020\n",
      "2020\n",
      "2018\n",
      "2018\n",
      "2016\n",
      "2016\n",
      "2018\n",
      "2019\n",
      "2019\n",
      "2019\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2020\n",
      "2021\n",
      "2020\n",
      "2020\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2020\n",
      "2013\n",
      "2015\n",
      "2015\n",
      "2015\n",
      "2014\n",
      "2014\n",
      "2013\n",
      "2013\n",
      "2013\n",
      "2015\n",
      "2015\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2015\n",
      "2012\n",
      "2013\n",
      "2013\n",
      "2012\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2013\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2014\n",
      "2013\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2013\n",
      "2014\n",
      "2012\n",
      "2015\n",
      "2014\n",
      "2013\n",
      "2013\n",
      "2012\n",
      "2012\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2015\n",
      "2015\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2014\n",
      "2014\n",
      "2012\n",
      "2012\n",
      "2015\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2014\n",
      "2014\n",
      "2015\n",
      "2013\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2012\n",
      "2015\n",
      "2014\n",
      "2013\n",
      "2019\n",
      "2017\n",
      "2018\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2019\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2018\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2016\n",
      "2016\n",
      "2016\n",
      "2018\n",
      "2016\n",
      "2018\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2020\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2016\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2016\n",
      "2018\n",
      "2018\n",
      "2017\n",
      "2019\n",
      "2020\n",
      "2016\n",
      "2019\n",
      "2017\n",
      "2017\n",
      "2020\n",
      "2020\n",
      "2020\n",
      "2017\n",
      "2017\n",
      "2017\n",
      "2020\n",
      "2020\n",
      "2016\n",
      "2019\n",
      "2019\n",
      "2019\n",
      "2019\n",
      "2018\n",
      "2018\n",
      "2017\n",
      "2019\n",
      "2020\n",
      "2019\n",
      "2020\n",
      "2019\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2016\n",
      "2019\n",
      "2016\n",
      "2019\n",
      "2016\n",
      "2020\n",
      "2018\n",
      "2020\n",
      "2019\n",
      "2017\n",
      "2020\n",
      "2020\n",
      "2019\n",
      "2016\n",
      "2020\n",
      "2017\n",
      "2020\n",
      "2020\n",
      "2017\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2016\n",
      "2017\n",
      "2016\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2019\n",
      "2018\n",
      "2018\n",
      "2020\n",
      "2020\n",
      "2020\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2016\n",
      "2017\n",
      "2017\n",
      "2019\n",
      "2019\n",
      "2016\n",
      "2016\n",
      "2020\n",
      "2020\n",
      "2018\n",
      "2018\n",
      "2016\n",
      "2016\n",
      "2018\n",
      "2019\n",
      "2019\n",
      "2019\n",
      "2019\n",
      "2018\n",
      "2016\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2020\n",
      "2021\n",
      "2020\n",
      "2020\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2020\n",
      "2021\n",
      "2021\n",
      "2020\n",
      "2020\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>the minister for national development (a) what...</td>\n",
       "      <td>[the, minister, for, national, development, a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>the minister for national development in view ...</td>\n",
       "      <td>[the, minister, for, national, development, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>: mr speaker, with your permission, i will tak...</td>\n",
       "      <td>[mr, speaker, with, your, permission, i, will,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>both the private property and hdb resale marke...</td>\n",
       "      <td>[both, the, private, property, and, hdb, resal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>nonetheless, we have seen an uptick in rpi in ...</td>\n",
       "      <td>[nonetheless, we, have, seen, an, uptick, in, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19839</th>\n",
       "      <td>23780</td>\n",
       "      <td>asked the acting minister for manpower how man...</td>\n",
       "      <td>[asked, the, acting, minister, for, manpower, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840</th>\n",
       "      <td>23783</td>\n",
       "      <td>, around 110,000 full-time employed residents ...</td>\n",
       "      <td>[around, 110000, fulltime, employed, residents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19841</th>\n",
       "      <td>23785</td>\n",
       "      <td>asked the acting minister for manpower (a) how...</td>\n",
       "      <td>[asked, the, acting, minister, for, manpower, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19842</th>\n",
       "      <td>23786</td>\n",
       "      <td>: based on reports by today and lianhe zaobao ...</td>\n",
       "      <td>[based, on, reports, by, today, and, lianhe, z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19843</th>\n",
       "      <td>23787</td>\n",
       "      <td>like all companies, the irs are subject to the...</td>\n",
       "      <td>[like, all, companies, the, irs, are, subject,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19844 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                                  0  \\\n",
       "0          2  the minister for national development (a) what...   \n",
       "1          5  the minister for national development in view ...   \n",
       "2          8  : mr speaker, with your permission, i will tak...   \n",
       "3          9  both the private property and hdb resale marke...   \n",
       "4         10  nonetheless, we have seen an uptick in rpi in ...   \n",
       "...      ...                                                ...   \n",
       "19839  23780  asked the acting minister for manpower how man...   \n",
       "19840  23783  , around 110,000 full-time employed residents ...   \n",
       "19841  23785  asked the acting minister for manpower (a) how...   \n",
       "19842  23786  : based on reports by today and lianhe zaobao ...   \n",
       "19843  23787  like all companies, the irs are subject to the...   \n",
       "\n",
       "                                                   clean  \n",
       "0      [the, minister, for, national, development, a,...  \n",
       "1      [the, minister, for, national, development, in...  \n",
       "2      [mr, speaker, with, your, permission, i, will,...  \n",
       "3      [both, the, private, property, and, hdb, resal...  \n",
       "4      [nonetheless, we, have, seen, an, uptick, in, ...  \n",
       "...                                                  ...  \n",
       "19839  [asked, the, acting, minister, for, manpower, ...  \n",
       "19840  [around, 110000, fulltime, employed, residents...  \n",
       "19841  [asked, the, acting, minister, for, manpower, ...  \n",
       "19842  [based, on, reports, by, today, and, lianhe, z...  \n",
       "19843  [like, all, companies, the, irs, are, subject,...  \n",
       "\n",
       "[19844 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd12 = []\n",
    "for file in scan_folder('/Users/Administrator/Desktop/UChicago/Y1Q3/30200/Project/hansard_data/hansard_full 2/'):\n",
    "    if file[98:102] == '2012': \n",
    "        f = open(file, 'r')\n",
    "        lis = [x.lower().replace('\\xa0','').replace('\\n','').split(':  ') for x in f.readlines()]\n",
    "        f.close()\n",
    "        pd12.append(pd.DataFrame(lis))\n",
    "    else:\n",
    "        continue\n",
    "    test = clean(pd12)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd12 = []\n",
    "pd13 = []\n",
    "pd14 = []\n",
    "pd15 = []\n",
    "pd16 = []\n",
    "pd17 = []\n",
    "pd18 = []\n",
    "pd19 = []\n",
    "pd20 = []\n",
    "pd21 = []\n",
    "massive = []\n",
    "def create_df(files):\n",
    "    for file in tqdm(files):\n",
    "        if file[98:102] == '2012': \n",
    "            f = open(file, 'r')\n",
    "            lis = [x.lower().replace('\\xa0','').replace('\\n','').split(':  ') for x in f.readlines()]\n",
    "            f.close()\n",
    "            pd12.append(pd.DataFrame(lis))\n",
    "        if file[98:102] == '2013': \n",
    "            f = open(file, 'r')\n",
    "            lis = [x.lower().replace('\\xa0','').replace('\\n','').split(':  ') for x in f.readlines()]\n",
    "            f.close()\n",
    "            pd13.append(pd.DataFrame(lis))\n",
    "        if file[98:102] == '2014':\n",
    "            f = open(file, 'r')\n",
    "            lis = [x.lower().replace('\\xa0','').replace('\\n','').split(':  ') for x in f.readlines()]\n",
    "            f.close()\n",
    "            pd14.append(pd.DataFrame(lis))\n",
    "        if file[98:102] == '2015':\n",
    "            f = open(file, 'r')\n",
    "            lis = [x.lower().replace('\\xa0','').replace('\\n','').split(':  ') for x in f.readlines()]\n",
    "            f.close()\n",
    "            pd15.append(pd.DataFrame(lis))\n",
    "        if file[98:102] == '2016':\n",
    "            f = open(file, 'r')\n",
    "            lis = [x.lower().replace('\\xa0','').replace('\\n','').split(':  ') for x in f.readlines()]\n",
    "            f.close()\n",
    "            pd16.append(pd.DataFrame(lis))\n",
    "        if file[98:102] == '2017':\n",
    "            f = open(file, 'r')\n",
    "            lis = [x.lower().replace('\\xa0','').replace('\\n','').split(':  ') for x in f.readlines()]\n",
    "            f.close()\n",
    "            pd17.append(pd.DataFrame(lis))\n",
    "        if file[98:102] == '2018':\n",
    "            f = open(file, 'r')\n",
    "            lis = [x.lower().replace('\\xa0','').replace('\\n','').split(':  ') for x in f.readlines()]\n",
    "            f.close()\n",
    "            pd18.append(pd.DataFrame(lis))\n",
    "        if file[98:102] == '2019':\n",
    "            f = open(file, 'r')\n",
    "            lis = [x.lower().replace('\\xa0','').replace('\\n','').split(':  ') for x in f.readlines()]\n",
    "            f.close()\n",
    "            pd19.append(pd.DataFrame(lis))\n",
    "        if file[98:102] == '2020':\n",
    "            f = open(file, 'r')\n",
    "            lis = [x.lower().replace('\\xa0','').replace('\\n','').split(':  ') for x in f.readlines()]\n",
    "            f.close()\n",
    "            pd20.append(pd.DataFrame(lis))\n",
    "        if file[98:102] == '2021':\n",
    "            f = open(file, 'r')\n",
    "            lis = [x.lower().replace('\\xa0','').replace('\\n','').split(':  ') for x in f.readlines()]\n",
    "            f.close()\n",
    "            pd21.append(pd.DataFrame(lis))\n",
    "        \n",
    "    massive.append(clean(pd12))\n",
    "    massive.append(clean(pd13))\n",
    "    massive.append(clean(pd14))\n",
    "    massive.append(clean(pd15))\n",
    "    massive.append(clean(pd16))\n",
    "    massive.append(clean(pd17))\n",
    "    massive.append(clean(pd18))\n",
    "    massive.append(clean(pd19))\n",
    "    massive.append(clean(pd20))\n",
    "    massive.append(clean(pd21))\n",
    "    \n",
    "    for i, file in zip(range(12,22), massive): \n",
    "        file.to_csv('/Users/Administrator/Desktop/UChicago/Y1Q3/30200/Project/hansard_data/hansard_full 2/{}.csv'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1255/1255 [00:03<00:00, 337.36it/s]\n"
     ]
    }
   ],
   "source": [
    "create_df(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012</td>\n",
       "      <td>['the', 'minister', 'for', 'national', 'develo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012</td>\n",
       "      <td>['the', 'minister', 'for', 'national', 'develo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012</td>\n",
       "      <td>['mr', 'speaker', 'with', 'your', 'permission'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012</td>\n",
       "      <td>['both', 'the', 'private', 'property', 'and', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012</td>\n",
       "      <td>['nonetheless', 'we', 'have', 'seen', 'an', 'u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189935</th>\n",
       "      <td>2021</td>\n",
       "      <td>['askedthe', 'minister', 'for', 'national', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189936</th>\n",
       "      <td>2021</td>\n",
       "      <td>['the', 'land', 'parcel', 'at', 'the', 'juncti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189937</th>\n",
       "      <td>2021</td>\n",
       "      <td>['askedthe', 'minister', 'for', 'national', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189938</th>\n",
       "      <td>2021</td>\n",
       "      <td>['housing', 'developers', 'licensed', 'under',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189939</th>\n",
       "      <td>2021</td>\n",
       "      <td>['lta', 'works', 'with', 'developers', 'to', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1189940 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         year                                              clean\n",
       "0        2012  ['the', 'minister', 'for', 'national', 'develo...\n",
       "1        2012  ['the', 'minister', 'for', 'national', 'develo...\n",
       "2        2012  ['mr', 'speaker', 'with', 'your', 'permission'...\n",
       "3        2012  ['both', 'the', 'private', 'property', 'and', ...\n",
       "4        2012  ['nonetheless', 'we', 'have', 'seen', 'an', 'u...\n",
       "...       ...                                                ...\n",
       "1189935  2021  ['askedthe', 'minister', 'for', 'national', 'd...\n",
       "1189936  2021  ['the', 'land', 'parcel', 'at', 'the', 'juncti...\n",
       "1189937  2021  ['askedthe', 'minister', 'for', 'national', 'd...\n",
       "1189938  2021  ['housing', 'developers', 'licensed', 'under',...\n",
       "1189939  2021  ['lta', 'works', 'with', 'developers', 'to', '...\n",
       "\n",
       "[1189940 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches = 0\n",
    "words = 0\n",
    "final_df = pd.DataFrame(columns = ['year', 'clean'])\n",
    "for i in range(12,22):\n",
    "    file = pd.read_csv('/Users/Administrator/Desktop/UChicago/Y1Q3/30200/Project/hansard_data/hansard_full 2/data/{}.csv'.\n",
    "                         format(i))\n",
    "    file['year'] = int('20{}'.format(i))\n",
    "    file = file[['year', 'clean']]\n",
    "    final_df = pd.concat([final_df, file], ignore_index = True)\n",
    "\n",
    "final_df\n",
    "    \n",
    "# test = pd.read_csv('/Users/Administrator/Desktop/UChicago/Y1Q3/30200/Project/hansard_data/hansard_full 2/21.csv')\n",
    "# test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "#     punc = '!()-[]{};:'\"\\,<>./?@#$%^&*_~'\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    df = df.dropna()\n",
    "    df['clean'] = df[df.iloc[:,0].str.len() >= 20]\n",
    "    df['clean'] = df['clean'].apply(lambda x: x.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "                                    .split() if type(x)==str else x).values\n",
    "    \n",
    "    return df.dropna().reset_index()\n",
    "#     df.to_csv('/Users/Administrator/Desktop/UChicago/Y1Q3/30200/Project/hansard_data/hansard_full 2/{}'.format(df[3:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing Divergence code\n",
    "def kl_divergence(X, Y):\n",
    "    P = X.copy()\n",
    "    Q = Y.copy()\n",
    "    P.columns = ['P']\n",
    "    Q.columns = ['Q']\n",
    "    df = Q.join(P).fillna(0)\n",
    "    p = df.iloc[:,1]\n",
    "    q = df.iloc[:,0]\n",
    "    D_kl = scipy.stats.entropy(p, q)\n",
    "    return D_kl\n",
    "\n",
    "def chi2_divergence(X,Y):\n",
    "    P = X.copy()\n",
    "    Q = Y.copy()\n",
    "    P.columns = ['P']\n",
    "    Q.columns = ['Q']\n",
    "    df = Q.join(P).fillna(0)\n",
    "    p = df.iloc[:,1]\n",
    "    q = df.iloc[:,0]\n",
    "    return scipy.stats.chisquare(p, q).statistic\n",
    "\n",
    "def Divergence(corpus1, corpus2, difference=\"KL\"):\n",
    "    \"\"\"Difference parameter can equal KL, Chi2, or Wass\"\"\"\n",
    "    freqP = nltk.FreqDist(corpus1)\n",
    "    P = pd.DataFrame(list(freqP.values()), columns = ['frequency'], index = list(freqP.keys()))\n",
    "    freqQ = nltk.FreqDist(corpus2)\n",
    "    Q = pd.DataFrame(list(freqQ.values()), columns = ['frequency'], index = list(freqQ.keys()))\n",
    "    if difference == \"KL\":\n",
    "        return kl_divergence(P, Q)\n",
    "    elif difference == \"Chi2\":\n",
    "        return chi2_divergence(P, Q)\n",
    "    elif difference == \"KS\":\n",
    "        try:\n",
    "            return scipy.stats.ks_2samp(P['frequency'], Q['frequency']).statistic\n",
    "        except:\n",
    "            return scipy.stats.ks_2samp(P['frequency'], Q['frequency'])\n",
    "    elif difference == \"Wasserstein\":\n",
    "        try:\n",
    "            return scipy.stats.wasserstein_distance(P['frequency'], Q['frequency'], u_weights=None, v_weights=None).statistic\n",
    "        except:\n",
    "            return scipy.stats.wasserstein_distance(P['frequency'], Q['frequency'], u_weights=None, v_weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating corpora for KL Divergence\n",
    "corpora = []\n",
    "for index, row in test.iterrows():\n",
    "    if len(corpora) > 10:\n",
    "        break\n",
    "    corpora.append(row['normalized_KL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sorted_df(df, category):\n",
    "    cats = sorted(set(df[category]))\n",
    "    new_df = pd.DataFrame(index = cats, columns = ['normalized'])\n",
    "    for cat in tqdm(cats):\n",
    "        #This can take a while\n",
    "        print(\"Embedding {}\".format(cat), end = '\\r')\n",
    "        subsetDF = df[df[category] == cat]\n",
    "        vocabulary = subsetDF['normalized_KL'].sum()\n",
    "        print(new_df)\n",
    "        new_df.loc[cat]['normalized'] = vocabulary\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "for p in tqdm(corpora):\n",
    "    l = []\n",
    "    for q in corpora:\n",
    "        l.append(Divergence(p,q, difference = 'KL'))\n",
    "    L.append(l)\n",
    "M = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pd.DataFrame(M, columns = fileids, index = fileids)\n",
    "ax = sns.heatmap(div, annot=True)\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "ax.set_title(\"Yearly KL-Divergence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_syn0norm(model):\n",
    "    \"\"\"since syn0norm is now depricated\"\"\"\n",
    "    return (model.wv.vectors / np.sqrt((model.wv.vectors ** 2).sum(-1))[..., np.newaxis]).astype(np.float32)\n",
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "    \"\"\"\n",
    "    Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf\n",
    "    Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "\n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "\n",
    "    # patch by Richard So [https://twitter.com/richardjeanso) (thanks!) to update this code for new version of gensim\n",
    "    # base_embed.init_sims(replace=True)\n",
    "    # other_embed.init_sims(replace=True)\n",
    "\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "    # get the (normalized) embedding matrices\n",
    "    base_vecs = calc_syn0norm(in_base_embed)\n",
    "    other_vecs = calc_syn0norm(in_other_embed)\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs)\n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v)\n",
    "    # Replace original array with modified one, i.e. multiplying the embedding matrix by \"ortho\"\n",
    "    other_embed.wv.vectors = (other_embed.wv.vectors).dot(ortho)\n",
    "\n",
    "    return other_embed\n",
    "\n",
    "def intersection_align_gensim(m1, m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.index_to_key)\n",
    "    vocab_m2 = set(m2.wv.index_to_key)\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1 & vocab_m2\n",
    "    if words: common_vocab &= set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1 - common_vocab and not vocab_m2 - common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, \"count\") + m2.wv.get_vecattr(w, \"count\"), reverse=True)\n",
    "    # print(len(common_vocab))\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1, m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.key_to_index[w] for w in common_vocab]\n",
    "        old_arr = m.wv.vectors\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.vectors = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        new_key_to_index = {}\n",
    "        new_index_to_key = []\n",
    "        for new_index, key in enumerate(common_vocab):\n",
    "            new_key_to_index[key] = new_index\n",
    "            new_index_to_key.append(key)\n",
    "        m.wv.key_to_index = new_key_to_index\n",
    "        m.wv.index_to_key = new_index_to_key\n",
    "\n",
    "        print(len(m.wv.key_to_index), len(m.wv.vectors))\n",
    "\n",
    "    return (m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeTokens(word_list, extra_stop=[]):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "    normalized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list]) \n",
    "\n",
    "    doc = nlp(word_list.lower())\n",
    "    \n",
    "    # add the property of stop word to words considered as stop words\n",
    "    if len(extra_stop) > 0:\n",
    "        for stopword in extra_stop:\n",
    "            lexeme = nlp.vocab[stopword]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "    for w in doc:\n",
    "        # if it's not a stop word or punctuation mark, add it to our article\n",
    "        if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # we add the lematized version of the word\n",
    "            normalized.append(str(w.lemma_))\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projections \n",
    "morality_words = pd.read_csv('/home/valalvern/models/morality_pairs.csv')\n",
    "word = 'migrantworker'\n",
    "moral_words = [x for x in morality_words['moral_eng'] if pd.isnull(x) == False]\n",
    "immoral_words = [x for x in morality_words['immoral_eng'] if pd.isnull(x) == False]\n",
    "\n",
    "safe = ['secure', 'safe', 'sound', 'harmless', 'innocuous', 'benign', 'wholesome', 'mild', 'guard', 'shield', 'shielded', 'guarded', 'secured']\n",
    "unsafe = ['insecure', 'unsafe', 'unsound', 'threat', 'harmful', 'hostile', 'vulnerable', 'reckless', 'dangerous', 'threatening', 'risky', 'prevarious', 'unpredictable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_year(embeddingsDict, migrant):\n",
    "    '''\n",
    "    construct affluence, morality, and status dimensions, and\n",
    "    project 'democracy' on these dimensions for each years\n",
    "    '''\n",
    "    m = []\n",
    "    s = []\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    for catOuter in cats:\n",
    "        print(catOuter)\n",
    "        #embeddings_aligned[catOuter] = [embeddings_raw[catOuter]]\n",
    "        #for embed in embeddingsDict[catOuter][1:]:\n",
    "        embed = embeddingsDict[catOuter][0]\n",
    "        security = dimension(embed, safe, unsafe)\n",
    "        morality = dimension(embed, moral_words, immoral_words)\n",
    "        try:\n",
    "            m.append(cosine_similarity(embed.wv[migrant].reshape(1, -1), morality.reshape(1,-1))[0][0])\n",
    "            s.append(cosine_similarity(embed.wv[migrant].reshape(1, -1), security.reshape(1, -1))[0][0])\n",
    "        except:\n",
    "            print(catOuter)\n",
    "    projection_df = pd.DataFrame({'morality' : m, 'security': s}, index = [i for i in cats]\n",
    "                                  )\n",
    "    return projection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_projection(projection, title):\n",
    "    #plt.plot(projection_df_safety, label='safety')\n",
    "    plt.plot(projection['morality'], label = 'morality')\n",
    "    plt.plot(projection['security'], label = 'security')\n",
    "    plt.title('{} Projection'.format(title))\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Projection')\n",
    "    plt.legend(loc=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(projection_df_li, 'littleindia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
